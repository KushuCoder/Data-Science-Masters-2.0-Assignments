{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. What is the main difference between the Euclidean distance metric and the Manhattan distance\n",
    "metric in KNN? How might this difference affect the performance of a KNN classifier or regressor?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "The main difference between Euclidean distance and Manhattan distance in KNN lies in how the distances between data points are calculated:     \n",
    "Euclidean Distance: Measures the straight-line (or \"as the crow flies\") distance between two points. It is calculated as the square root of the sum of the squared differences between corresponding coordinates.    \n",
    "Formula: square root of(sigma(x_i - y_i)^2)     \n",
    "Manhattan Distance: Measures the sum of the absolute differences between the coordinates. It calculates the total movement required to reach from one point to another, only considering horizontal and vertical paths.    \n",
    "Formula: sigma|x_i-y_i|     \n",
    "Impact on KNN:     \n",
    "Euclidean distance tends to work better when the data points are spread out in a continuous space and have smooth boundaries.    \n",
    "Manhattan distance works better when the data lies on a grid or the relationship between variables is non-continuous (e.g., in urban grid-like structures).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. How do you choose the optimal value of k for a KNN classifier or regressor? What techniques can be\n",
    "used to determine the optimal k value?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "The optimal value of 'K' in KNN can be chosen through a process called hyperparameter tuning. Common techniques for selecting 'K' include:    \n",
    "1.Cross-Validation: Divide the dataset into several folds and train the model on different folds, evaluating the error rate for each K value.     \n",
    "2.Grid Search: Test a range of values for K and select the one with the best performance on a validation set.    \n",
    "3.Elbow Method: Plot the error rate (or accuracy) for different K values and look for the 'elbow' point, which indicates the optimal K.      \n",
    "4.Bias-Variance Tradeoff: A smaller K will have high variance (overfitting), and a larger K will have high bias (underfitting). You can choose the optimal K by balancing these two effects.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. How does the choice of distance metric affect the performance of a KNN classifier or regressor? In\n",
    "what situations might you choose one distance metric over the other?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "The choice of distance metric significantly affects the performance of a KNN classifier or regressor. Each metric calculates distances differently and can impact the results:\n",
    "- Euclidean Distance: Works well when the data is continuous, and the relationships between variables are smooth.\n",
    "- Manhattan Distance: Often preferred in grids or discrete systems, as it measures movements along axes rather than straight-line distances.\n",
    "- Minkowski Distance: A generalized version of Euclidean and Manhattan distance, where the order of distance (p) can be tuned.\n",
    "- Cosine Similarity: Sometimes used in text classification, it measures the angle between two vectors, rather than the straight-line distance.\n",
    "\n",
    "Choosing the right metric:\n",
    "- For data where relationships are continuous and smooth, Euclidean distance is often best.\n",
    "- For data with grid-like structures or discrete variables, Manhattan distance may perform better.\n",
    "- Experimenting with different metrics and evaluating the performance on validation data is key to choosing the best one for a given problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. What are some common hyperparameters in KNN classifiers and regressors, and how do they affect\n",
    "the performance of the model? How might you go about tuning these hyperparameters to improve\n",
    "model performance?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Some common hyperparameters in KNN classifiers and regressors include:\n",
    "1. K: The number of nearest neighbors considered for classification or regression. A small K may overfit the model, while a large K may underfit it.\n",
    "2. Distance Metric: The method used to calculate the distance between points (e.g., Euclidean, Manhattan, etc.).\n",
    "3. Weighting of Neighbors: KNN can assign weights to neighbors (e.g., uniform or distance-based weights). Distance-based weighting can make the prediction more sensitive to closer points.\n",
    "4. Algorithm: The algorithm used for computing the nearest neighbors (e.g., brute-force, KD-tree, or ball-tree).\n",
    "\n",
    "Hyperparameter tuning can be performed through techniques like grid search or random search to find the best combination of parameters that minimize the error and improve model performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. How does the size of the training set affect the performance of a KNN classifier or regressor? What\n",
    "techniques can be used to optimize the size of the training set?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "The size of the training set directly impacts the performance of KNN models:    \n",
    "- Smaller Training Set: Leads to faster computations but may result in a less generalized model, as there are fewer examples for the algorithm to learn from.    \n",
    "- Larger Training Set: Increases computational time, but it can improve the model's performance by providing more diverse data for training.    \n",
    "\n",
    "Techniques to optimize the training set size:     \n",
    "1. Sampling: Use techniques like random sampling or stratified sampling to balance the size of the training set and reduce variance.    \n",
    "2. Dimensionality Reduction: Use methods like PCA (Principal Component Analysis) to reduce the number of features, making the model faster and less prone to overfitting.    \n",
    "3. Efficient Data Structures: Use KD-trees or Ball-trees to speed up the computation when handling large datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6. What are some potential drawbacks of using KNN as a classifier or regressor? How might you\n",
    "overcome these drawbacks to improve the performance of the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Potential drawbacks of using KNN include:    \n",
    "1. High Computational Cost: KNN can be slow during prediction time, especially for large datasets, since it computes distances to every data point.    \n",
    "2. Curse of Dimensionality: In high-dimensional spaces, the concept of distance becomes less meaningful, and KNN performance can degrade.     \n",
    "3. Sensitivity to Noise: KNN is sensitive to noisy data and outliers since it treats all neighbors equally.    \n",
    "\n",
    "Ways to address these drawbacks:     \n",
    "1. Dimensionality Reduction: Use PCA or other feature selection techniques to reduce the feature space and mitigate the curse of dimensionality.    \n",
    "2. Data Structures: Use KD-trees or Ball-trees for faster nearest neighbor search.    \n",
    "3. Preprocessing: Apply data cleaning, outlier detection, and noise reduction techniques to reduce the impact of noisy data.    \n",
    "4. Weighted KNN: Apply distance-based weighting for neighbors to prioritize closer neighbors and reduce the impact of outliers.    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
