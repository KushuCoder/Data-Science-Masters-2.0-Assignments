{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. What is the KNN algorithm?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "The K-Nearest Neighbors (KNN) algorithm is used for classification and regression tasks. In KNN, the prediction for a new data point is made based on the majority class (for classification) or average value (for regression) of its 'K' nearest neighbors, determined by a distance metric (e.g., Euclidean or Manhattan distance)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. How do you choose the value of K in KNN?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "The value of K represents the number of nearest neighbors to consider when making a prediction. The optimal value of K depends on the dataset.      Typically, you can choose K using cross-validation and evaluating the modelâ€™s performance. A common approach is:    \n",
    "1. Test different K values.    \n",
    "2. Select the K that minimizes the mean squared error.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. What is the difference between KNN classifier and KNN regressor?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "KNN Classifier: Used for classification tasks, where the goal is to assign a class label to a data point. It classifies based on the majority vote from the K nearest neighbors.\n",
    "KNN Regressor: Used for regression tasks, where the goal is to predict a continuous value. It predicts by taking the average (or weighted average) of the values of the K nearest neighbors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. How do you measure the performance of KNN?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "The performance of a KNN model can be measured based on the following:\n",
    "For Classification:\n",
    "1. Accuracy: Percentage of correctly predicted labels.\n",
    "2. Precision, Recall, and F1-Score: Metrics for imbalanced datasets.\n",
    "3. Confusion Matrix: Helps in analyzing the true positives, false positives, true negatives, and false negatives.\n",
    "\n",
    "For Regression:\n",
    "1. Mean Absolute Error (MAE): Average of the absolute differences between actual and predicted values.\n",
    "2. Mean Squared Error (MSE): Average of the squared differences between actual and predicted values.\n",
    "3. R-squared : Measures how well the model explains the variance in the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. What is the curse of dimensionality in KNN?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "The curse of dimensionality refers to the challenges posed by high-dimensional spaces when applying distance-based algorithms like KNN. As the number of dimensions increases, the data points become sparse, and the concept of \"nearness\" becomes less meaningful. This leads to reduced model performance, increased computational cost, and the potential for overfitting due to irrelevant features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6. How do you handle missing values in KNN?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "To handle missing values in KNN:\n",
    "1. Imputation: Replace missing values with a measure like the mean, median, or mode of the feature.\n",
    "2. KNN Imputation: Use the KNN algorithm itself to predict missing values based on the nearest neighbors.\n",
    "3. Remove Data Points: If the missing values are few, remove the data points with missing values.\n",
    "4. Feature Engineering: Sometimes, creating a new feature indicating whether the value is missing can help the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for\n",
    "which type of problem?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "KNN Classifier: Best suited for categorical target variables. It works well when class boundaries are well-defined and there is sufficient data.    \n",
    "Strength: Simplicity, non-parametric, no assumption about the distribution of the data.    \n",
    "Weakness: Can struggle with high-dimensional data, computationally expensive with large datasets.    \n",
    "KNN Regressor: Best suited for continuous target variables. It works well when there are smooth transitions in the data.    \n",
    "Strength: Simple and easy to interpret for regression tasks.    \n",
    "Weakness: It can suffer from overfitting in noisy data, and performance degrades with high dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks,\n",
    "and how can these be addressed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Strengths:    \n",
    "No assumptions about the underlying data distribution.    \n",
    "Easy to understand and implement.    \n",
    "Can be used for both classification and regression tasks.    \n",
    "\n",
    "Weaknesses:    \n",
    "KNN requires distance computations for each prediction, making it slow for large datasets.    \n",
    "KNN can be heavily influenced by outliers.    \n",
    "Performance deteriorates in high-dimensional spaces due to the curse of dimensionality.    \n",
    "\n",
    "To address these weaknesses:    \n",
    "Reduce dimensionality using techniques like PCA.    \n",
    "Use efficient data structures like KD-trees for faster nearest neighbor searches.    \n",
    "Handle noisy data by applying pre-processing techniques like outlier removal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "- Euclidean Distance: Measures the straight-line distance between two points in Euclidean space. It is calculated as the square root of the sum of the squared differences of the coordinates:\n",
    "  square root of(sigma(xi-yi)^2)\n",
    "\n",
    "- Manhattan Distance: Measures the sum of the absolute differences of the coordinates. It is calculated as:\n",
    "  sigma mod of(xi-yi)\n",
    "\n",
    "The main difference is that Euclidean distance considers the direct path between points, while Manhattan distance considers only horizontal and vertical movements, which can be useful in grids or other constrained environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q10. What is the role of feature scaling in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Feature scaling is crucial in KNN because the algorithm relies on distance metrics to determine the proximity of data points. Features with larger scales can dominate the distance calculations, leading to biased predictions. Scaling the features ensures that all features contribute equally to the distance computation. Common scaling techniques include Min-Max scaling and Standardization (Z-score scaling)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
