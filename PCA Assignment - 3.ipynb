{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. What are Eigenvalues and Eigenvectors? How are they related to the Eigen-Decomposition approach?\n",
    "Explain with an example.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Eigenvalues and eigenvectors used to analyze matrices and their transformations.    \n",
    "Eigenvalue: A scalar value lambda such that for a given square matrix A and a non-zero vector v, the equation Av = lambda*v holds true.    \n",
    "Eigenvector: A non-zero vector v that, when multiplied by the matrix A, results in a scaled version of itself, that is, Av = lambda*v.    \n",
    "\n",
    "Eigen-Decomposition:    \n",
    "Eigen-decomposition is a method of decomposing a matrix A into eigenvectors and eigenvalues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. What is eigen decomposition and what is its significance in linear algebra?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Eigen decomposition is the process of decomposing a square matrix into its eigenvalues and eigenvectors.    \n",
    "Significance:    \n",
    "Eigen-decomposition allows us to diagonalize a matrix (if it is diagonalizable). This is useful for simplifying matrix operations, such as matrix exponentiation or solving differential equations.     \n",
    "It provides insight into how the matrix transforms vectors, particularly in terms of stretching or compressing along certain directions (eigenvectors)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. What are the conditions that must be satisfied for a square matrix to be diagonalizable using the\n",
    "Eigen-Decomposition approach? Provide a brief proof to support your answer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Condition to be satisfied for Diagonalizability:    \n",
    "A matrix A is diagonalizable if it has n linearly independent eigenvectors (where n is the size of the matrix).    \n",
    "If A has n linearly independent eigenvectors, it can be expressed as A = QΛQ^-1, where Q is the matrix of eigenvectors and Λ is a diagonal matrix of eigenvalues.\n",
    "Proof:\n",
    "1. If A has n linearly independent eigenvectors, we can construct a matrix Q where each column is an eigenvector.\n",
    "2. The matrix Q is invertible if its columns are linearly independent.\n",
    "3. The matrix Λ is a diagonal matrix of eigenvalues corresponding to the eigenvectors in Q.\n",
    "4. Thus, A = QΛQ⁻¹ is a diagonalized form of A."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. What is the significance of the spectral theorem in the context of the Eigen-Decomposition approach?\n",
    "How is it related to the diagonalizability of a matrix? Explain with an example.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Significance:     \n",
    "The spectral theorem ensures that symmetric matrices are diagonalizable, and their eigenvectors are orthogonal.     \n",
    "This property is significant in fields such as physics, machine learning, and statistics, as it simplifies matrix operations and allows for efficient computation.    \n",
    "Example:    \n",
    "Consider a symmetric matrix:    \n",
    "A = [[4, 1],\n",
    "     [1, 3]]    \n",
    "This matrix can be diagonalized using the spectral theorem. The eigenvectors are orthogonal, and the eigenvalues are the diagonal elements of the matrix Λ, which can be computed by solving the characteristic equation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. How do you find the eigenvalues of a matrix and what do they represent?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "To find the eigenvalues of a matrix A, we solve the characteristic equation:    \n",
    "det(A - λI) = 0    \n",
    "The eigenvalues represent the scaling factor by which the corresponding eigenvectors are stretched or compressed during the transformation.    \n",
    "A positive eigenvalue indicates that the corresponding eigenvector is stretched.    \n",
    "A negative eigenvalue indicates that the corresponding eigenvector is reversed.    \n",
    "An eigenvalue of zero indicates that the transformation collapses the vector along that eigenvector direction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6. What are eigenvectors and how are they related to eigenvalues?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "\n",
    "Eigenvectors are non-zero vectors that only get scaled (stretched or compressed) when a linear transformation is applied to them.    \n",
    "Relation to Eigenvalues:    \n",
    "When a matrix A is multiplied by an eigenvector v, the result is a scaled version of the eigenvector: A * v = λ * v.     \n",
    "The eigenvalue λ is the scalar factor by which the eigenvector is scaled.     \n",
    "Each eigenvector corresponds to a specific eigenvalue that describes how much the transformation stretches or compresses the vector along that direction.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7. Can you explain the geometric interpretation of eigenvectors and eigenvalues?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Geometrically, eigenvectors represent the directions in which a linear transformation (such as a matrix multiplication) acts by stretching or compressing, but not changing the direction of the vector.    \n",
    "Eigenvectors: They represent the axes along which the data is transformed, either stretched or compressed, without rotation.    \n",
    "Eigenvalues: The eigenvalue indicates how much the vector is stretched (if the eigenvalue is greater than 1) or compressed (if the eigenvalue is between 0 and 1). If the eigenvalue is negative, the direction of the eigenvector is reversed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8. What are some real-world applications of eigen decomposition?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Eigen decomposition has various real-world applications, including:\n",
    "\n",
    "1. Principal Component Analysis (PCA)    \n",
    "2. Quantum Mechanics    \n",
    "3. Image Compression    \n",
    "4. Graph Theory    \n",
    "5. Face Recognition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q9. Can a matrix have more than one set of eigenvectors and eigenvalues?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "No, a matrix can only have one unique set of eigenvalues and eigenvectors, but they may have multiple eigenvectors corresponding to the same eigenvalue, especially in cases where the eigenvalue is repeated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q10. In what ways is the Eigen-Decomposition approach useful in data analysis and machine learning?\n",
    "Discuss at least three specific applications or techniques that rely on Eigen-Decomposition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "1. Principal Component Analysis (PCA): PCA relies on eigen-decomposition to identify the directions (principal components) in which the data has the most variance. This helps in reducing dimensionality while retaining the most important features of the data.\n",
    "2. Singular Value Decomposition (SVD): SVD, used in techniques like Latent Semantic Analysis (LSA) and collaborative filtering, is based on the eigen-decomposition of matrices to extract features and reduce noise in high-dimensional data.\n",
    "3. Spectral Clustering: Eigen-decomposition is used in spectral clustering to find clusters by analyzing the eigenvectors of the similarity matrix, enabling the identification of groups in the data based on the graph structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
